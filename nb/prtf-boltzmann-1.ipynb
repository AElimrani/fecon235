{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boltzmann portfolios\n",
    "\n",
    "### Part 1: Single decision relationship to Markowitz\n",
    "\n",
    "We develop an alternative to the traditional mean-variance\n",
    "framework (\"*Markowitz*\" portfolios) called\n",
    "***Boltzmann*** portfolios which addresses uncertainty\n",
    "from the standpoint of entropy and optimal sequential decisions.\n",
    "The result is a faster online algorithm which is more robust\n",
    "and has no dependencies on offline convex optimization packages.\n",
    "\n",
    "Boltzmann portfolios rely on geometric mean returns since they \n",
    "optimally express mean-variance under logarithmic utility. \n",
    "Accuracy has been improved by using our research on\n",
    "Gaussian mixtures presented in https://git.io/gmix --\n",
    "specifically, our function gemrat() computes the geometric mean\n",
    "rate by taking into account the fourth central moment, kurtosis.\n",
    "This is crucial for *leptokurtotic* (\"fat-tailed\") assets.\n",
    "\n",
    "After individual geometric mean rates of the underlying assets\n",
    "have been estimated, we use the covariance matrix to inform\n",
    "us of possible inter-correlations we can exploit the minimize\n",
    "the variance of the Boltzmann portfolio.\n",
    "Such minimization helps to maximize the portfolio's return.\n",
    "\n",
    "Markowitz portfolios are optimal in the arithmetic mean-variance\n",
    "framework for a *single-period*. They are fragile to changing\n",
    "market conditions, much like elegant battle strategies which\n",
    "crumble under harsh war conditions.\n",
    "In contrast, Boltzmann portfolios are designed to be\n",
    "adaptive over multiple periods to maximize final wealth.\n",
    "Techniques have been borrowed from Bayesian and\n",
    "reinforcement learning.\n",
    "\n",
    "[Sequential decision-making using Boltzmann porfolios\n",
    "will be covered in another notebook (Part ?).]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Dependencies:*\n",
    "\n",
    "- Repository: https://github.com/rsvp/fecon235\n",
    "     \n",
    "*CHANGE LOG*\n",
    "\n",
    "    2017-06-28  Refactor subroutines to lib/ys_prtf_boltzmann.py\n",
    "    2017-06-27  Rewrite relying more arrays than lists, for speed later.\n",
    "    2017-06-26  Rough draft, test subroutines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from fecon235.fecon235 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ::  Python 2.7.13\n",
      " ::  IPython 5.1.0\n",
      " ::  jupyter_core 4.2.1\n",
      " ::  notebook 4.1.0\n",
      " ::  matplotlib 1.5.1\n",
      " ::  numpy 1.11.0\n",
      " ::  scipy 0.17.0\n",
      " ::  sympy 1.0\n",
      " ::  pandas 0.19.2\n",
      " ::  pandas_datareader 0.2.1\n",
      " ::  Repository: fecon235 v5.17.0603 devPrtf\n",
      " ::  Timestamp: 2017-06-27T18:52:12Z\n",
      " ::  $pwd: /media/yaya/virt15h/virt/dbx/Dropbox/ipy/fecon235/nb\n"
     ]
    }
   ],
   "source": [
    "#  PREAMBLE-p6.15.1223d :: Settings and system details\n",
    "from __future__ import absolute_import, print_function, division\n",
    "system.specs()\n",
    "pwd = system.getpwd()   # present working directory as variable.\n",
    "print(\" ::  $pwd:\", pwd)\n",
    "#  If a module is modified, automatically reload it:\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#       Use 0 to disable this feature.\n",
    "\n",
    "#  Notebook DISPLAY options:\n",
    "#      Represent pandas DataFrames as text; not HTML representation:\n",
    "import pandas as pd\n",
    "pd.set_option( 'display.notebook_repr_html', False )\n",
    "from IPython.display import HTML # useful for snippets\n",
    "#  e.g. HTML('<iframe src=http://en.mobile.wikipedia.org/?useformat=mobile width=700 height=350></iframe>')\n",
    "from IPython.display import Image \n",
    "#  e.g. Image(filename='holt-winters-equations.png', embed=True) # url= also works\n",
    "from IPython.display import YouTubeVideo\n",
    "#  e.g. YouTubeVideo('1j_HxD4iLn8', start='43', width=600, height=400)\n",
    "from IPython.core import page\n",
    "get_ipython().set_hook('show_in_pager', page.as_hook(page.display_page), 0)\n",
    "#  Or equivalently in config file: \"InteractiveShell.display_page = True\", \n",
    "#  which will display results in secondary notebook pager frame in a cell.\n",
    "\n",
    "#  Generate PLOTS inside notebook, \"inline\" generates static png:\n",
    "%matplotlib inline   \n",
    "#          \"notebook\" argument allows interactive zoom and resize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "\n",
    "Traditional portfolio selection (see References) develop\n",
    "what is generally known as the \"**efficient frontier**\"\n",
    "pictured here:  \n",
    "\n",
    "![Efficient mean-variance frontier](http://webpage.pace.edu/pviswanath/notes/investments/gif/assetalloc77.gif)\n",
    "\n",
    "Please pay particular attention on the apex of the hyperbolic curve \n",
    "(the \"bullet\") which indicates the point of **global minimum variance**.\n",
    "\n",
    "A student may first believe the frontier's usefulness\n",
    "in portfolio management, but later as a professional will\n",
    "voice something along these lines:\n",
    "\n",
    "> What I now find problematic is this theoretical concept that one\n",
    "needs to only pick a point along the top of the bullet to achieve\n",
    "the optimal balance between risk and reward.\n",
    "Yeah, like it's so easy to just slide your finger along the curve\n",
    "and select at your leisure what you'd like to earn on your money.\n",
    "The focus on past prices that make up these curves\n",
    "and the assumption that the covariance structure is stationary\n",
    "in perpetuity is so wrong in practice.\n",
    "What is rarely talked about is that the future will\n",
    "NOT fall along that line. It is nice to know what you\n",
    "could have earned, but what you will earn surely will not be so\n",
    "easily cherry-picked off of some rear-view historic curve.\n",
    "--(edited Market Tech comment on 2015-04-10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data and construct a dataframe\n",
    "\n",
    "We retrieve the following data of daily frequency\n",
    "representing equities worldwide and gold by five ETF securities: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'America': 's4spy',\n",
       " 'Emerging': 's4eem',\n",
       " 'Europe': 's4ezu',\n",
       " 'Gold': 's4gld',\n",
       " 'Japan': 's4ewj'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Convenient dictionary set in fecon235.py,\n",
    "#  where key is world region, and value is its fecon235 data code:\n",
    "world4d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#  Or manually specify your own dictionary here:\n",
    "prices_dic = world4d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ::  Retrieved from Google Finance: SPY\n",
      " ::  Retrieved from Google Finance: EEM\n",
      " ::  Retrieved from Google Finance: EZU\n",
      " ::  Retrieved from Google Finance: GLD\n",
      " ::  Retrieved from Google Finance: EWJ\n"
     ]
    }
   ],
   "source": [
    "#  Download data into a dataframe, alphabetically by key:\n",
    "prices = groupget( prices_dic, maxi=3650 )\n",
    "#  ... about ten years worth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========= Specify DATES for  [start:end] =========\n",
    "\n",
    "We can analyze different epochs of history\n",
    "by executing **\"Cell\" > \"Run All Below\"** from the Jupyter menu,\n",
    "*without downloading the data again*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  Set the start and end dates:\n",
    "start = '2007-01-01'\n",
    "end = '2100-01-01'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary statistics for prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           America     Emerging       Europe         Gold        Japan\n",
      "count  2513.000000  2513.000000  2513.000000  2513.000000  2513.000000\n",
      "mean    156.098743    39.844282    37.192829   120.000330    44.313796\n",
      "std      43.186322     6.005217     8.096488    26.186519     6.094609\n",
      "min      68.110000    18.260000    20.320000    64.420000    27.480000\n",
      "25%     122.760000    37.200000    32.310000   104.080000    39.160000\n",
      "50%     145.070000    40.620000    35.710000   118.830000    44.880000\n",
      "75%     198.460000    43.410000    39.440000   133.740000    48.880000\n",
      "max     244.660000    55.670000    63.440000   184.590000    59.120000\n",
      "\n",
      " ::  Index on min:\n",
      "America    2009-03-09\n",
      "Emerging   2008-11-20\n",
      "Europe     2009-03-09\n",
      "Gold       2007-07-05\n",
      "Japan      2009-03-09\n",
      "dtype: datetime64[ns]\n",
      "\n",
      " ::  Index on max:\n",
      "America    2017-06-19\n",
      "Emerging   2007-10-31\n",
      "Europe     2007-10-31\n",
      "Gold       2011-08-22\n",
      "Japan      2007-07-13\n",
      "dtype: datetime64[ns]\n",
      "\n",
      " ::  Head:\n",
      "            America  Emerging  Europe   Gold  Japan\n",
      "T                                                  \n",
      "2007-07-02   151.79     44.72   59.74  65.02  59.08\n",
      "2007-07-03   152.34     45.08   60.08  64.74  58.96\n",
      "2007-07-05   152.18     45.28   59.82  64.42  58.80\n",
      " ::  Tail:\n",
      "            America  Emerging  Europe    Gold  Japan\n",
      "T                                                   \n",
      "2017-06-22   242.84     41.28   40.34  118.92  54.09\n",
      "2017-06-23   243.13     41.50   40.39  119.43  54.05\n",
      "2017-06-26   243.29     41.90   40.54  118.36  53.90\n",
      "\n",
      " ::  Correlation matrix:\n",
      "           America  Emerging    Europe      Gold     Japan\n",
      "America   1.000000  0.073680  0.172430  0.067585  0.697921\n",
      "Emerging  0.073680  1.000000  0.626293  0.230620  0.414527\n",
      "Europe    0.172430  0.626293  1.000000 -0.514633  0.758757\n",
      "Gold      0.067585  0.230620 -0.514633  1.000000 -0.391059\n",
      "Japan     0.697921  0.414527  0.758757 -0.391059  1.000000\n"
     ]
    }
   ],
   "source": [
    "stats( prices[start:end] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geometric mean rate\n",
    "\n",
    "David E. Shaw, famous for his proprietary hedge fund, remarked that \n",
    "one of the most important equations in finance is the penalization \n",
    "of arithmetic mean by one-half of variance:\n",
    "\n",
    "$$ g = \\mu - \\frac{\\sigma^2}{2} $$\n",
    "\n",
    "which turns out to be a second-order approximation of geometric mean rate.\n",
    "It is good enough to maximize, before considering \n",
    "intricate mean-variance trade-offs.\n",
    "\n",
    "However, many assets have leptokurtotic returns (\"fat-tails\") and so\n",
    "a better approximation for the geometric mean rate is needed\n",
    "which takes into account the fourth central moment called *kurtosis*.\n",
    "Details are given on Gaussian mixtures in our research\n",
    "at https://git.io/gmix\n",
    "\n",
    "For maximimizing wealth over many periods, the geometric mean rate\n",
    "is a far more reliable metric than the arithmetic mean rate.\n",
    "\n",
    "A Boltzmann portfolio maximizes the weighted geometric mean rate\n",
    "of its underlying assets.\n",
    "\n",
    "The source code shows that groupgemrat() gives us \n",
    "the **geometric** mean return, followed by \n",
    "the **arithmetic mean return, volatility, and Pearson kurtosis**, \n",
    "then yearly frequency, sample size, and key -- in list format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2.08, 4.81, 20.93, 16.09, 256, 2512, 'America'],\n",
       " [-11.02, -0.66, 33.25, 17.26, 256, 2512, 'Emerging'],\n",
       " [-10.55, -3.95, 29.76, 10.04, 256, 2512, 'Europe'],\n",
       " [4.01, 6.1, 19.69, 9.32, 256, 2512, 'Gold'],\n",
       " [-5.04, -0.94, 23.66, 16.92, 256, 2512, 'Japan']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Geometric mean rates, non-overlapping, annualized:\n",
    "gems = groupgemrat( prices[start:end], yearly=256 )\n",
    "gems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fourth element in each list gives us the kurtosis statistic\n",
    "where 3 is theoretically expected if the distribution is Gaussian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Portfolio details\n",
    "\n",
    "Single agent? Just pick the one with highest geometric mean rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['America', 'Emerging', 'Europe', 'Gold', 'Japan']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  By construction, keys will be alphabetically sorted:\n",
    "keys = [ item[-1] for item in gems ]\n",
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2.08],\n",
       "       [-11.02],\n",
       "       [-10.55],\n",
       "       [  4.01],\n",
       "       [ -5.04]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Gather just the geometric mean rates into an array:\n",
    "rates = np.array([item[0] for item in gems]).reshape(len(gems), 1)\n",
    "rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#  Compute the covariance matrix:\n",
    "V = covdiflog( prices[start:end] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.    0.88  0.87  0.02  0.77]\n",
      " [ 0.88  1.    0.84  0.14  0.75]\n",
      " [ 0.87  0.84  1.    0.12  0.74]\n",
      " [ 0.02  0.14  0.12  1.    0.06]\n",
      " [ 0.77  0.75  0.74  0.06  1.  ]]\n"
     ]
    }
   ],
   "source": [
    "#  The covariance matrix itself is hard to interpret,\n",
    "#  so show the Pearson correlation coefficients,\n",
    "#  rounded to n decimal places:\n",
    "print( cov2cor(V, n=2) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This correlation matrix pertains to the differential\n",
    "between logged prices, *not* the prices themselves.\n",
    "\n",
    "As expected, the equities group is inter-correlated,\n",
    "whereas Gold rates stand apart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights from covariance matrix\n",
    "\n",
    "We now turn our attention to the weights associated with the **Global\n",
    "Minimum Variance Portfolio**. Its derivation is found in Cochrane\n",
    "(2005), chp. 5, p.83:\n",
    "\n",
    "$$ \\mathbf{w} = \\frac{V^{-1}\\mathbf{1}} { \\mathbf{1}^\\top V^{-1} \\mathbf{1} } $$\n",
    "\n",
    "Note that the weights are solely dependent on the covariance matrix $V$.\n",
    "There are no constraints involved.\n",
    "\n",
    "The Boltzmann portfolio is *informed* by the Lagrangian formulation\n",
    "of the covariance structure, not in the static single-period sense,\n",
    "but rather in its dynamic evolution over time.\n",
    "Weights can be revised by an online algorithm which\n",
    "tracks covariance, e.g. the Kalman filter, without\n",
    "performing quadratic optimization offline.\n",
    "\n",
    "We shall take only what we truly need in terms of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##  Uncomment to see defined function...\n",
    "#  weighcov??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.90353325],\n",
       "       [-0.34943601],\n",
       "       [-0.19642015],\n",
       "       [ 0.46541758],\n",
       "       [ 0.17690532]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "globalw = weighcov( V )\n",
    "globalw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights at this point are for the\n",
    "*Global Minimum Variance Porfolio* (GMVP).\n",
    "Negative weights imply trading short in the market.\n",
    "\n",
    "We mark our debt to Harry Markowitz,\n",
    "see his further work on constrainted portfolios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deciding on the expert(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.87934917],\n",
       "       [ 3.8507848 ],\n",
       "       [ 2.07223259],\n",
       "       [ 1.8663245 ],\n",
       "       [-0.89160283]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Using arrays, we are multiplying element-wise:\n",
    "scores = globalw * rates\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.7770882295247112"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  GLOBAL portfolio return with UNRESTRICTED short sales:\n",
    "np.sum(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights indicating short positions are usually\n",
    "paired with negative geometric returns.\n",
    "\n",
    "Interesting that America had the highest geometric mean return,\n",
    "but now considering the covariance matrix,\n",
    "Emerging has the best score (but involving shorting it)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with possible short sales\n",
    "\n",
    "Negative weights imply the underlying assets should be shorted.\n",
    "A Boltzmann portfolio only considers the weights as ***advisory***.\n",
    "\n",
    "We may want to limit short sales at -0.3 weight, or perhaps ignore tiny\n",
    "positions for rebalancing purposes in a multiple-period setting.\n",
    "\n",
    "So we specify a threshold weight, and renormalize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MIN_weight = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mtrimit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m For an iterable, accept values >= floor, else set to level.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/Dropbox/ipy/fecon235/lib/ys_prtf_boltzmann.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trimit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.90353325],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.46541758],\n",
       "       [ 0.17690532]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = trimit( globalw, MIN_weight, 0 )\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.58448728],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.30107431],\n",
       "       [ 0.11443841]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = renormalize(weights)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.21573353],\n",
       "       [-0.        ],\n",
       "       [-0.        ],\n",
       "       [ 1.207308  ],\n",
       "       [-0.57676959]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_trim = weights * rates\n",
    "scores_trim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8462719335475208"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Portfolio return with trimmed weights:\n",
    "np.sum(scores_trim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing Boltzmann and softmax\n",
    "\n",
    "Suppose you have n classes. For some feature, you want to estimate its\n",
    "probabilities of being in class i. However, your algorithm does not\n",
    "directly produce probabilities -- instead it first produces real-valued scores.\n",
    "From these scores you can define the probabilities using the\n",
    "softmax function. We work with functions that map to the unconstrained space\n",
    "of scores, and then map those scores to the space of probability vectors.\n",
    "This allows us to divide up the problem into n subproblems of predicting\n",
    "scores. One can view this as a generalization of logistic regression.\n",
    "\n",
    "In statistical physics, the softmax function happens to be the probability of\n",
    "an atom being found in a quantum state of energy when the atom is part of an\n",
    "ensemble that has reached thermal equilibrium.\n",
    "This is known as the **Boltzmann** distribution.\n",
    "\n",
    "In the field of reinforcement learning, the softmax function is used to\n",
    "convert values into action probabilities.\n",
    "A positive parameter T called the temperature is introduced\n",
    "to divide through each value. It is a scaling operation such that\n",
    "high temperatures cause corresponding actions to be equi-probable.\n",
    "Low temperatures cause a greater difference in selection probability\n",
    "for actions that differ in their value estimates.\n",
    "At low temperatures, the probability of the action with the\n",
    "highest expected reward tends to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  The temperature is a hyperparameter which should be varied:\n",
    "TEMPERATURE = 55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.4651],\n",
       "       [ 0.0342],\n",
       "       [ 0.0342],\n",
       "       [ 0.4567],\n",
       "       [ 0.0099]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "problist = softmax( scores_trim, temp=TEMPERATURE )[-1]\n",
    "probs = np.array( problist ).reshape(len(problist), 1)\n",
    "probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Want to calculate new trimmed weights based on softmax adjustments.\n",
    "\n",
    "EXPLAIN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.27184503],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.13750064],\n",
       "       [ 0.00113294]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trimprobs = weights * probs\n",
    "trimprobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Renormalization is necessary, but also trimit() is helpful,\n",
    "followed by renormalize() again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.66409651],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.33590349],\n",
       "       [ 0.        ]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trimprobs = renormalize(trimit(renormalize(trimprobs), MIN_weight, 0))\n",
    "trimprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.38132074],\n",
       "       [-0.        ],\n",
       "       [-0.        ],\n",
       "       [ 1.34697299],\n",
       "       [-0.        ]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_soft = trimprobs * rates\n",
    "scores_soft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.7282937329110535"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Portfolio return for Boltzmann portfolio:\n",
    "np.sum(scores_soft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def boltzmann_cap( capital, probs, rates, keys ):\n",
    "    '''Recap porfolio numerically -- only within this notebook.'''\n",
    "    for i, p in enumerate(probs):\n",
    "        print( keys[i], round(p*capital, 0) )\n",
    "    print(\"_________\")\n",
    "    scores = probs * rates\n",
    "    print(\"PORTFOLIO geometric mean return:\", round(np.sum(scores), 2), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "America 664.0\n",
      "Emerging 0.0\n",
      "Europe 0.0\n",
      "Gold 336.0\n",
      "Japan 0.0\n",
      "_________\n",
      "PORTFOLIO geometric mean return: 2.73 %\n"
     ]
    }
   ],
   "source": [
    "#  Allot across per thousand in capital:\n",
    "boltzmann_cap(1000, trimprobs, rates, keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "```\n",
    "prices ---> cov ---> globalw\n",
    "  |                    |\n",
    "  |                  trimit\n",
    "  |                  renormalize\n",
    "  |                    |\n",
    "  |                    |\n",
    "groupgemrat          weights\n",
    "  |                    |\n",
    "  |____________________|\n",
    "             |\n",
    "  temp --> softmax --> probs --> trimprobs --> Boltzmann\n",
    "             \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "- John H. Cochrane, 2005 revised ed., *Asset Pricing*, Princeton U. Press.\n",
    "\n",
    "- On softmax:\n",
    "    - https://en.wikipedia.org/wiki/Softmax_function \n",
    "    - http://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative\n",
    "    - http://cs231n.github.io/linear-classify/#softmax \n",
    "    - https://en.wikipedia.org/wiki/Reinforcement_learning "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
